{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1pSA5iIuz6o1GUOQbPUSAV2oTWsG2psIk",
      "authorship_tag": "ABX9TyOhyqUbY0zrQRY0tzanWQWd",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AhmedAboushanab/Genomic-Insights-into-Specialized-and-Primary-Metabolism-Gene-Clusters/blob/main/Data_Extraction_from_antiSMASH.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install beautifulsoup4 pandas"
      ],
      "metadata": {
        "id": "xjAx0qn30eSF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import hashlib\n",
        "from bs4 import BeautifulSoup, Tag\n",
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "def generate_row_hash(entry: dict) -> str:\n",
        "    \"\"\"Generate a unique hash for a dictionary of row values.\"\"\"\n",
        "    row_str = \"|\".join(str(value) for value in entry.values())\n",
        "    return hashlib.md5(row_str.encode(\"utf-8\")).hexdigest()\n",
        "\n",
        "def extract_antismash_data(html_path, sample_id):\n",
        "    with open(html_path, 'r', encoding='utf-8') as f:\n",
        "        soup = BeautifulSoup(f, 'html.parser')\n",
        "\n",
        "    start_heading = soup.find(string=re.compile(\"Identified secondary metabolite regions.*strictness\"))\n",
        "    end_heading = soup.find(string=re.compile(\"No secondary metabolite regions were found\"))\n",
        "\n",
        "    content_blocks = []\n",
        "    current = start_heading\n",
        "    while current and current != end_heading:\n",
        "        content_blocks.append(current)\n",
        "        current = current.next_element\n",
        "\n",
        "    tables = []\n",
        "    seen_table_ids = set()\n",
        "    for el in content_blocks:\n",
        "        if isinstance(el, Tag) and el.name == \"table\":\n",
        "            node_id_tag = el.find_previous(string=re.compile(\"NODE_\"))\n",
        "            node_id = node_id_tag.strip() if node_id_tag else None\n",
        "            if node_id and node_id not in seen_table_ids:\n",
        "                tables.append(el)\n",
        "                seen_table_ids.add(node_id)\n",
        "    records = []\n",
        "    seen_hashes = set()\n",
        "\n",
        "    for table in tables:\n",
        "        node_label = \"\"\n",
        "        prev = table\n",
        "        while prev and not node_label:\n",
        "            prev = prev.find_previous(string=True)\n",
        "            if prev and \"NODE\" in prev:\n",
        "                node_label = prev.strip()\n",
        "\n",
        "        for row in table.find_all(\"tr\")[1:]:\n",
        "            cols = row.find_all(\"td\")\n",
        "            region = re.sub(r'\\s+', ' ', cols[0].get_text(strip=True).replace(u'\\xa0', ' ')) if len(cols) > 0 else \"\"\n",
        "            bgc_type = cols[1].get_text(strip=True) if len(cols) > 1 else \"\"\n",
        "            from_coord = cols[2].get_text(strip=True).replace(\",\", \"\") if len(cols) > 2 else \"\"\n",
        "            to_coord = cols[3].get_text(strip=True).replace(\",\", \"\") if len(cols) > 3 else \"\"\n",
        "\n",
        "            similar_lines = []\n",
        "            if len(cols) > 5:\n",
        "                similar_lines = list(cols[4].stripped_strings) + list(cols[5].stripped_strings)\n",
        "            elif len(cols) == 5:\n",
        "                similar_lines = list(cols[4].stripped_strings)\n",
        "\n",
        "            cluster_name = similar_lines[0] if len(similar_lines) > 0 else \"\"\n",
        "            cluster_class = similar_lines[1] if len(similar_lines) > 1 else \"\"\n",
        "\n",
        "            similarity_text = cols[6].get_text(strip=True) if len(cols) > 6 else \"\"\n",
        "            similarity_pct = re.search(r\"(\\d+)%\", similarity_text)\n",
        "            similarity_value = similarity_pct.group(1) if similarity_pct else \"\"\n",
        "\n",
        "            row_data = {\n",
        "                \"Sample-id\": sample_id,\n",
        "                \"Node\": node_label,\n",
        "                \"Region\": region,\n",
        "                \"Type\": bgc_type,\n",
        "                \"From\": from_coord,\n",
        "                \"To\": to_coord,\n",
        "                \"Most_similar_known cluster\": cluster_name,\n",
        "                \"Similarity %\": similarity_value,\n",
        "                \"cluster\": cluster_class\n",
        "            }\n",
        "\n",
        "            row_hash = generate_row_hash(row_data)\n",
        "            if row_hash in seen_hashes:\n",
        "                continue\n",
        "            seen_hashes.add(row_hash)\n",
        "\n",
        "            records.append(row_data)\n",
        "\n",
        "    return pd.DataFrame(records)\n",
        "\n",
        "def process_antismash_folder(folder_path):\n",
        "    all_records = []\n",
        "    count_per_sample = {}\n",
        "\n",
        "    for root, _, files in os.walk(folder_path):\n",
        "        for file in files:\n",
        "            if file.endswith(\".html\"):\n",
        "                html_path = os.path.join(root, file)\n",
        "                sample_id = os.path.splitext(file)[0]\n",
        "                print(f\"Processing {sample_id}\")\n",
        "                df = extract_antismash_data(html_path, sample_id)\n",
        "                count_per_sample[sample_id] = len(df)\n",
        "                all_records.append(df)\n",
        "\n",
        "    all_records = [df for df in all_records if not df.empty]\n",
        "\n",
        "    if all_records:\n",
        "        combined_df = pd.concat(all_records, ignore_index=True)\n",
        "        combined_df = combined_df.drop_duplicates()  # Just in case\n",
        "        combined_df.to_csv(\"antismash_combined_output.csv\", index=False)\n",
        "        print(\"\\nâœ… antismash_combined_output.csv saved.\")\n",
        "\n",
        "        print(\"\\nðŸ“Š BGC Count per Sample:\")\n",
        "        for sample, count in count_per_sample.items():\n",
        "            print(f\"  {sample}: {count} regions\")\n",
        "        return combined_df\n",
        "    else:\n",
        "        print(\"No data extracted from any HTML files.\")\n",
        "        return pd.DataFrame()\n"
      ],
      "metadata": {
        "id": "zvv2y2bsJlx2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "process_antismash_folder(\"/content/drive/MyDrive/BGC-HTML-Files /References\")"
      ],
      "metadata": {
        "id": "78jdf3sDJoPx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}